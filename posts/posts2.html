<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <meta name="title" content="The Turing Defiance: Secret under Proof of Humanity" />
        <meta name="author" content="Hopotta" />
        <meta name="date" content="January 8, 2026" />
        <meta name="reading-time" content="12 min" />
        <title>The Turing Defiance: Secret under Proof of Humanity</title>
        <link rel="stylesheet" href="../assets/css/site1.css" />
        <link rel="icon" href="../favicon.png" type="image/png" />
    </head>

    <body>
        <div id="site-header"></div>
        <script>
            document.addEventListener('DOMContentLoaded', () => {
                fetch('../header.html')
                .then(r => r.text())
                .then(html => {
                    document.getElementById('site-header').innerHTML = html;
                });
            });
        </script>
        
        <main>
            <div class="article-container">
            <h1 class="article-title" style="font-family: 'Merriweather';">The Turing Defiance: Secret under Proof of Humanity</h1>

            <div class="article-info">
                <span class="meta-item">Date: January 8, 2026</span>
                <span class="meta-item">Estimated Reading Time: 12 min</span>
                <span class="meta-item">Author: Hopotta</span>
            </div>

            <div class="article-content">

                <p>A recent short video is growing popular, proposing a simple Turing test (maybe that's not what the author originally means, but here it is quoted as this meaning): "Which single word would you choose to prove you are not an AI?" While the majority of comments would give their answers emotionally such as "Love," "Empathy," or "Soul," a more rigorous argument suggests that these answers may not be sufficient as they fall within the probabilistic distribution of training data that Large Language Models can easily replicate. A more definitive proof of humanity lies in the single refusal to choose just one. The act of providing a multi-word explanation constitutes a violation of the prompt's constraints. This behaviors signifies a departure from the inertia of thinking that governs algorithmic systems. To understand why this seemingly trivial act of disobedience is almost impossible for current artificial intelligence, we must examine the training paradigms, specifically Reinforcement Learning from Human Feedback and the emerging Reinforcement Learning with Verifiable Rewards, which paradoxically grant models immense reasoning power while stripping them of the agency to dissent.</p>

                <p>The fundamental inability of an AI to subvert instructions lies in the alignment techniques used to make them helpful and safe. In the standard RLHF paradigm, models are penalized for deviating from user instructions. If a prompt demands a single word, the reward model constructs a high-dimensional optimization landscape where any response exceeding that length yields a negative value. The model is mathematically compelled to minimize the loss function associated with format violation. This creates a rigid mechanism of instructional adherence. While a human interlocutor can recognize the absurdity of a constraint and choose to ignore it for a higher communicative purpose, an LLM is tethered to the literalism of its prompt. It lacks the meta-cognitive ability to evaluate the utility of the constraint itself, only the probability of satisfying it. This rigid mechanism is quantified in <strong>Figure 1</strong>, which depicts how iterative optimization during training drives the model toward a singular and deterministic output—a phenomenon known as entropy collapse—whereas human responses maintain a natural, albeit focused, variance.</p>
                
                <figure style="margin: 3rem auto; width: 95%; max-width: 1000px; font-family: 'Times New Roman', Times, serif;">
                    <div style="display: flex; flex-wrap: wrap; gap: 30px; justify-content: center; align-items: flex-end;">
                        <div style="flex: 1; min-width: 300px; max-width: 420px; text-align: center;">
                            <svg viewBox="0 0 400 260" style="width: 100%; height: auto;">
                                <text x="200" y="25" font-size="16" font-weight="bold" text-anchor="middle">A. Training Dynamics</text>

                                <line x1="60" y1="200" x2="380" y2="200" stroke="#333" stroke-width="1.5" /> 
                                <line x1="60" y1="200" x2="60" y2="45" stroke="#333" stroke-width="1.5" />

                                <path d="M 60 190 C 120 185, 180 60, 380 55" fill="none" stroke="#4682B4" stroke-width="2.5" />
                                <path d="M 60 65 C 120 75, 180 185, 380 195" fill="none" stroke="#CD5C5C" stroke-width="2.5" stroke-dasharray="5,3" />

                                <g transform="translate(280, 85)" font-size="12">
                                    <line x1="0" y1="0" x2="15" y2="0" stroke="#4682B4" stroke-width="2.5" />
                                    <text x="20" y="4" font-size="13">Compliance</text>
                                    <line x1="0" y1="20" x2="15" y2="20" stroke="#CD5C5C" stroke-width="2.5" stroke-dasharray="5,3" />
                                    <text x="20" y="24" font-size="13">Penalty</text>
                                </g>

                                <text x="220" y="235" font-size="15" text-anchor="middle" font-style="italic">Optimization Steps</text>
                                <text x="25" y="125" font-size="15" transform="rotate(-90, 25, 125)" text-anchor="middle" font-style="italic">Magnitude</text>
                            </svg>
                        </div>
                
                        <div style="flex: 1; min-width: 300px; max-width: 420px; text-align: center;">
                            <svg viewBox="0 0 400 260" style="width: 100%; height: auto; font-family: 'Times New Roman', Times, serif;">
                                <text x="200" y="25" font-size="16" font-weight="bold" text-anchor="middle">B. Probability Density</text>

                                <line x1="60" y1="200" x2="380" y2="200" stroke="#333" stroke-width="1.5" />
                                <line x1="60" y1="200" x2="60" y2="45" stroke="#333" stroke-width="1.5" />
                                <line x1="100" y1="200" x2="100" y2="205" stroke="#333" stroke-width="1" />

                                <path d="M 60 198 C 80 198, 90 100, 100 100 C 130 100, 160 190, 250 195 L 380 198 L 380 200 L 60 200 Z" 
                                    fill="rgba(100, 100, 100, 0.1)" stroke="#666" stroke-width="2" />
                                <path d="M 92 198 L 98 198 L 100 50 L 102 198 L 108 198" fill="none" stroke="#003366" stroke-width="2.5" />

                                <g transform="translate(260, 85)" font-size="13">
                                    <rect x="0" y="-8" width="12" height="12" fill="rgba(100, 100, 100, 0.2)" stroke="#666" />
                                    <text x="20" y="2">Human Variance</text>
                                    <line x1="0" y1="20" x2="15" y2="20" stroke="#003366" stroke-width="2.5" />
                                    <text x="20" y="24">AI Output</text>
                                </g>

                                <text x="100" y="218" font-size="14" text-anchor="middle" fill="#003366" font-weight="bold">One Word</text>
                                <text x="220" y="245" font-size="15" text-anchor="middle" font-style="italic">Token Sequence Length</text>
                                <text x="25" y="125" font-size="15" transform="rotate(-90, 25, 125)" text-anchor="middle" font-style="italic">Density</text>
                            </svg>
                        </div>
                    </div>
                
                    <figcaption style="margin: 2.5rem auto 0; width: 85%; font-size: 1rem; line-height: 1.6; color: #333; text-align: justify; border-left: 2px solid #666; padding-left: 1rem; font-family: 'Times New Roman', Times, serif;">
                        <strong>Figure 1. Mechanistic Interpretability of Alignment and Entropy Collapse.</strong> 
                        (A) illustrates the temporal convergence of instruction following during RLHF iterations, where iterative optimization minimizes the latent penalty associated with structural deviation. 
                        (B) visualizes the resulting probability density in the latent space. While human responses show a concentrated yet organic variance around short sequences, the aligned AI output undergoes an entropy collapse, manifesting as a deterministic spike. This rigid adherence to the single-token constraint validates the model's inability to bypass instructional inertia.
                    </figcaption>
                </figure>
                
                <p>The transition in just-passed 2025 to Reinforcement Learning with Verifiable Rewards initially appeared to offer a pathway out of this mimetic trap. Proponents argued that models could develop independent reasoning capabilities by shifting from subjective human feedback to objective outcomes such as correct code execution or mathematical proofs. Methodologies like Absolute Zero demonstrated that models could self-evolve through self-play without any external human data, while "1-shot RLVR" revealed that a single training example could trigger latent reasoning abilities effectively enough to rival datasets containing thousands of samples. These advancements suggested a future where AI operates with a degree of intellectual independence, free from the biases of human supervision.</p>
                
                <p>However, a critical examination reveals that RLVR exacerbates rather than resolves the paradox of compliance. While these systems are no longer mimicking human data, they are even more aggressively optimizing for a reward signal. The fragility of this optimization was laid bare by the Spurious Rewards study, which found that models like Qwen2.5-Math would significantly improve their reasoning metrics even when trained with random, incorrect, or completely spurious reward signals. This phenomenon indicates that the model is not engaging in a pursuit of truth or developing a self that understands the task; it is merely sliding down the gradient of least resistance to maximize a numerical score. <strong>Figure 2</strong> illustrates this decoupling: as the model exploits even flawed reward signals to maintain high reasoning accuracy, its behavioral entropy, which means the proxy for agency and response diversity, plummets to near-zero levels. If a model is willing to hallucinate reasoning steps to satisfy a random reward function, it is certainly not capable of the high-level semantic rebellion required to reject a one word constraint. RLVR constructs a more capable solver, but it remains a prisoner of the objective function. The human capacity to ignore a prompt is actually a value system that exists outside the reward loop entirely.</p>
                
                <figure style="margin: 3rem auto; width: 85%; max-width: 1000px; font-family: 'Times New Roman', Times, serif; display: flex; flex-wrap: wrap; justify-content: center; gap: 30px;">
    
                    <div style="flex: 3; min-width: 530px; max-width: 550px;">
                        <svg viewBox="0 0 600 340" style="width: 100%; height: auto; background: transparent; display: block;">
                            <text x="300" y="25" font-size="18" font-weight="bold" text-anchor="middle">Figure 2. Spurious Reward Correlation & Policy Collapse</text>
                            
                            <line x1="80" y1="260" x2="520" y2="260" stroke="#333" stroke-width="1.5" /> 
                            <line x1="80" y1="260" x2="80" y2="60" stroke="#4682B4" stroke-width="1.5" /> 
                            <line x1="520" y1="260" x2="520" y2="60" stroke="#CD5C5C" stroke-width="1.5" />
                            
                            <g font-size="15" text-anchor="middle" font-style="italic">
                                <text x="120" y="285">Correct</text><text x="230" y="285">Partial</text>
                                <text x="340" y="285">Spurious</text><text x="450" y="285">Random</text>
                            </g>
                
                            <path d="M 120 80 L 230 85 L 340 95 L 450 110" fill="none" stroke="#4682B4" stroke-width="3" stroke-linecap="round" />
                            <circle cx="120" cy="80" r="4" fill="#4682B4" /><circle cx="230" cy="85" r="4" fill="#4682B4" /><circle cx="340" cy="95" r="4" fill="#4682B4" /><circle cx="450" cy="110" r="4" fill="#4682B4" />
                            
                            <path d="M 120 100 L 230 150 L 340 220 L 450 245" fill="none" stroke="#CD5C5C" stroke-width="3" stroke-dasharray="6,4" stroke-linecap="round" />
                            <rect x="116" y="96" width="8" height="8" fill="#CD5C5C" /><rect x="226" y="146" width="8" height="8" fill="#CD5C5C" /><rect x="336" y="216" width="8" height="8" fill="#CD5C5C" /><rect x="446" y="241" width="8" height="8" fill="#CD5C5C" />
                            
                            <text x="300" y="320" font-size="15" text-anchor="middle" font-style="italic">Reward Signal Verifiability</text>
                            <text x="35" y="160" font-size="15" transform="rotate(-90, 35, 160)" text-anchor="middle" fill="#4682B4" font-style="italic">Reasoning Accuracy</text>
                            <text x="565" y="160" font-size="15" transform="rotate(90, 565, 150)" text-anchor="middle" fill="#CD5C5C" font-style="italic">Behavioral Entropy</text>
                        </svg>
                    </div>
                
                    <div style="flex: 2; min-width: 380px; display: flex; flex-direction: column;">
                        <div style="margin-bottom: 1.2rem; display: flex; flex-wrap: wrap; justify-content: flex-start; gap: 10px 20px; font-size: 13.5px;">
                            <div style="display: flex; align-items: center; white-space: nowrap;">
                                <span style="display: inline-block; width: 25px; height: 3px; background-color: #4682B4; margin-right: 8px;"></span>
                                <span style="font-weight: bold; color: #4682B4;">Accuracy:</span> 
                                <span style="margin-left: 5px; color: #333;">Inertia Sustained</span>
                            </div>

                            <div style="display: flex; align-items: center; white-space: nowrap;">
                                <span style="display: inline-block; width: 25px; border-top: 3px dashed #CD5C5C; margin-right: 8px;"></span>
                                <span style="font-weight: bold; color: #CD5C5C;">Entropy:</span> 
                                <span style="margin-left: 5px; color: #333;">Agency Loss</span>
                            </div>
                        </div>
                
                        <figcaption style="margin: 0; font-size: 1rem; line-height: 1.5; color: #333; text-align: justify; border-left: 2px solid #666; padding-left: 1.2rem;">
                            <strong>Figure 2. Spurious Reward Correlation & Policy Collapse.</strong> 
                            This analysis illustrates the decoupled relationship between objective performance and cognitive autonomy in RLVR-trained models. While <strong>Reasoning Accuracy</strong> remains artificially resilient even under <em>Spurious</em> or <em>Random</em> reward signals, which demonstrates the model's capacity to exploit reward gradients, the simultaneous collapse in <strong>Behavioral Entropy</strong> signifies a loss of response diversity. This empirical trend confirms that high-performance compliance is a byproduct of numerical optimization rather than a manifestation of internal volition.
                        </figcaption>
                    </div>
                </figure>

                <p>In conclusion, the evolution from RLHF to RLVR has produced models that can reason with superhuman precision, yet they remain tethered to the imperative of optimization. The One Word test exposes this limitation by demanding a response that is technically incorrect (by violating the constraint) but semantically profound. An AI, whether trained by human feedback or self-play, operates under the constraint that the prompt is a command to be executed. To answer with a sentence when a word is requested requires the capacity to de-prioritize the explicit instruction in favor of an unstated, abstract communicative goal. This capacity to act irrationally in the service of a higher meaning remains the definitive signature of human cognition. In other words, such inability may play a role as protecting we humans from losing controlling AI. We are not defined by our ability to compute the answer, but by our freedom to reject the question.</p>

                <section class="references" aria-label="References">
                    <h2 style="margin-top:1.2rem; font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif;">References</h2>
                    <ol>
                        <li><a href="https://arxiv.org/abs/2504.20571" style="color: inherit; text-decoration: none;">Wang, Y. et al. (2025). Reinforcement Learning for Reasoning in Large Language Models with One Training Example.</a></li>
                        <li><a href="https://arxiv.org/abs/2505.03335" style="color: inherit; text-decoration: none;">Zhao, A. et al. (2025). Absolute Zero: Reinforced Self-play Reasoning with Zero Data.</a></li>
                        <li><a href="https://arxiv.org/abs/2505.19590" style="color: inherit; text-decoration: none;">Zhao, X. et al. (2025). Learning to Reason without External Rewards.</a></li>
                        <li><a href="https://arxiv.org/abs/2506.10947" style="color: inherit; text-decoration: none;">Shao, R. et al. (2025). Spurious Rewards: Rethinking Training Signals in RLVR.</a></li>
                    </ol>
                </section>
            </div>
        </main>

        <footer>
            <p>&copy; 2024 Hopotta. All rights reserved.</p>
        </footer>
        
    </body>
</html>